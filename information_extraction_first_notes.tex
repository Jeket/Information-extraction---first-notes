\documentstyle[12pt]{article}

\title{Information extraction --- first notes}
\author{Michael Nielsen}

\begin{document}

\maketitle

This document contains my first notes getting a survey overview of the
field of information extraction, including notions such as entity
recognition.  It's unrevised and undistilled, written solely for
myself, and unlikely to be of much use to anyone else.


\section{Overview}

\textbf{Wikipedia article on information extraction:} Example is
extracting reports of company mergers from news wire articles.  Early
example of a system was JASPER, built for Reuters, with the aim of
providing financial news to financial traders.  1987-1998: the Message
Understanding Conferences were seminal.  Support came from DARPA, who
wished to automate tasks performed by analysts.  It's unclear to me
how much the semantic web will help --- stuff in ``machine readable''
form often isn't that much easier to understand than plain text.
Standard way of thinking: unstructured data -> database.  There are
all sorts of subtasks: recognizing named entities; detecting
coferences; extracting relationships.  Not so dissimilar to full-on
natural language processing! Also includes things like table
extraction, comment extraction, and terminology extraction.
Information extraction is also done on media sources other than text.
Wrappers can be used to extract a particular page's content.  They're
usually taylored to highly structured collections of pages.

The article claims that there are \textbf{three standard approaches}
to information extraction: hand-written regular expressions;
classifiers (Bayes, Maxent); and sequence models.

\textbf{Wikipedia article on text simplification:} This is a procedure
one applies to a text.  The intent is to preserve meaning, while
making the text simpler.  It'd be interesting to have an example of
output from such a system.  One thing which is done is to break
complex compound sentences down into simple sentences, without
subordinate clauses.

At least some of this work is being used in science, to extract
information from papers.

The use of text simplification sets up a pipeline.  We take complex
sentences, then apply text simplification to produce simple output
sentences, which can then be understood using a further stage.  It's a
nice two-part model.  It's unclear to me that this is what we actually
do when we parse speech (not that that necessarily matters).  One of
the papers in the area (by Klebanov, Knight and Marcu) introduces a
notion of Easy Access Sentences: the goal of text simplification is to
produce such sentences.

\textbf{Wikipedia article on wrappers:} Says there are two main
approaches to wrappers: (1) wrapper induction, and (2) automated data
extraction.  Wrapper induction starts with a set of manually labelled
training examples to learn data extraction rules.  The problem, of
course, is that this requires one to come up with a set of training
examples, and the system will break when we see examples outside the
types seen in the training set.  This may occur, for example, when a
site changes its template.  Automated data extraction tries to
discover the patterns in webpages without manually labelled training
data.

\textbf{Wikipedia article on named entity recognition:} Also known as
entity identification and entity extraction.  The idea is to classify
atomic elements in text into predefined (really?) categories.  You can
think of it as a form of annotion, taking unstructured text, and
beginning to structure it.  For example, the Message Understanding
Conferences developed a set of standard tags (ENAMEX) that were used
to categorize the atomic elements in text.  This approach seems
surprising to me: it doesn't seem scalable, given the very large
(infinite) number of categories we humans potentially have available.
It's also true that one atomic element in text may belong to multiple
categories.

The article claims that the best current systems for named entity
recognition perform at near-human levels of performance.  But it also
comments that named entity recognition systems which perform well in
one domain often don't perform well in other domains.  As for machine
translation there is again a distinction between rule-based and
statistical model-based systems.  (I wonder which is better?)

At least two hierarchies of named entities have been proposed in the
literature.  There are BBN categories, with 29 types and 64 subtypes.
There is an extended hierarchy, due to Sekine, with 200 subtypes.

\textbf{Wikipedia article on Message Understanding Conferences:} The
conference was organized with many research teams competing against
one another, with a metrics-based approach.  At the sixth conference
--- MUC-6 --- the task of recognizing named entities and coreferences
was added.  For a named entity, all phrases in the text were ``to be
marked as person, location, organization, time or quantity.''  The MUC
conferences were associated with DARPA's TIPSTER program, which aimed
at doing document detection, information extraction, and
summarization.  TIPSTER was apparently discontinued in 1998.

\textbf{Wikipedia article on precision and recall:} I'm familiar with
these notions in the context of information retrieval.  But the
Wikipedia article does an interesting job broadening the discussion to
pattern recognition in general.  Starts with the notion of
\emph{accuracy}: how often is the correct results returned.  For
example, if we're shown an image of a whale, how often does an
algorithm correctly identify it as a whale?  Considers precision and
recall as generalizations.  I'll come back to this point.

Let's recap how it works in information retrieval.  You're given a
query, $q$, and identify some set $S$ of documents as relevant.  In
fact, the ``true'' set of relevant document is $S_T$.  The ``correct''
documents are those in $S \cap S_T$.  So \emph{recall} is the fraction
of all relevant documents returned: $|S \cap S_T| / |S_T|$.
\emph{Precision} is the fraction of document returned by the search
which are truly relevant: $|S \cap S_T| / |S|$.

In other words, recall is the answer to the question ``What fraction
of truly relevant documents to the query are returned?''  Precision is
the answer to the question ``What fraction of the returned documents
are truly relevant to the query?''

It's easy to build an information retrieval system which gets high
recall: it simply dumps everything.  Unfortunately, this will gives
low precision: most of the documents won't be truly relevant to the
query.

It's also (relatively) easy to build an information retrieval system
which gets high precision: it simply tries to identify a single
document which is highly likely to be relevant, and returns that.  

Note that the task of building high-precision systems is complicated
by the fact that some queries are going to be hard to understand.
Many people might guess that ``President in a black hat'' refers to
Abraham Lincoln, but an information retrieval system might have
trouble with this.  So it seems likely that we might get low-precision
results.  But a sufficiently generous search algorithm might also get
pretty high recall, since at least \emph{some} webpages will make the
connection between ``black hat'' and ``Lincoln'', and (hopefully) that
will allow it to infer that it should return other pages connected to
Lincoln.

It's worth pointing out that both precision and recall are
mathematically \emph{ill-defined}.  They depend on human psychology.
And to evaluate them will require training examples.  It is by no
means clear how one would do this at web scale.

We can think of spam in this context.  What spam does is pollutes your
search results, returning irrelevant pages, i.e., decreasing
precision.  But it doesn't necessarily affect recall.  When someone
says something is ``spammy'' that's really what is meant: it is
decreasing precision.  ``Good'' advertising doesn't do this: it only
shows you stuff that is relevant, while ``bad'' advertising decreases
precision.

It's interesting to consider that television advertising is very low
precision, while Google's advertising aims to be high precision: i.e.,
to only show you stuff that is actually relevant.

Coming back to the issue of evaluating precision, this is something
which in principle we ought to be able to estimate using user click
data.  It's not so clear, though, how we'd be able to estimate recall.
My intuition is that users will be much more sensitive to changes in
precision than to changes in recall.  

There is a use case where the last sentence in the last paragraph is
definitely not true: when you're trying to find something that you've
previously found.  In that case, there is only one relevant document,
and it should (ideally) be the single document returned.  At a minimum
you need perfect recall (and, ideally, high precision).


\textbf{Wikipedia article on coreference resolution:}

\textbf{Wikipedia article on relationship extraction:}

\textbf{Wikipedia article on terminology extraction:}

\textbf{Wikipedia article on supervised learning:}

\textbf{Wikipedia article on unsupervised learning:}

\textbf{Wikipedia article on naive Bayes:}

\textbf{Wikipedia article on Hidden Markov Model:}

\textbf{Wikipedia article on Conditional Random Fields:}

Other Wikipedia articles: concept mining; web scraping; nutch;
enterprise search; semantic translation; TIPSTER; faceted search;
natural language processing; Matthew Correlation Coefficient.

\section{Papers}

What are the key papers?

Open information extraction from the web:
http://portal.acm.org/citation.cfm?id=1409378

Information extraction (Sunita Sarawagi):
http://portal.acm.org/citation.cfm?id=1498845

Coupled semi-supervised learning for information extraction (Andrew
Carlson, Justin Betteride, Richard C. Wang, Estevam R. Hruschka, Jr,
and Tom M. Mitchell): http://portal.acm.org/citation.cfm?id=1718501

Open Information Extraction (Oren Etzioni):
http://oai.dtic.mil/oai/oai?verb=getRecord\&metadataPrefix=html\&identifier=ADA538482

\textbf{Motivations and methods for text simplification:}
(R. Chandresekar, Christine Doran, and B. Srinivas, 1996):
http://portal.acm.org/citation.cfm?id=993361.  This appears to be one
of the key early papers.  The idea is that instead of parsing a
sentence, we'll instead use an alternative approach which is simpler
than full parsing.  They present two approaches: (1) Use a finite
state group to produce noun and verb groups; (2) Use a supertagging
model to produce dependency linkages.

\textbf{Ralph Grisham and Beth Sundheim, Message Understanding
  Conference - 6: A Brief History:
  http://acl.ldc.upenn.edu/C/C96/C96-1079.pdf:} A metrics-based
approach to driving science.  It would be interesting to know how well
this worked (it's a very clear goal!) and what it missed (may have led
to a concentration of attention).  MUC-1 is described as being
exploratory.  MUC-2 crystallized things out.  The first metric is
\emph{recall}: what fraction of keys were filled out correctly?  And
\emph{precision:} what fraction of keys that were filled out were
filled out cooectly?  (Note that precision will always be better than
recall with this definition.)  This article suggests that it wasn't
until MUC-5 that it became part of DARPA TIPSTER.  Interestingly,
MUC-6 broadened the metrics, so that there were several tasks
participants could focus on.

\textbf{Banko and Brill:}

\section{Patents}


\section{Tools}

What are the standard tools?

\textbf{Apache UIMA:}

\textbf{AlchemyAPI:}

\textbf{NetOwl Extractor:}

\textbf{Tools on Github: matpalm/named-entity-extraction XXX}

\textbf{ReVerb:}

\textbf{GExp:}

\textbf{OpenCalais:}

\textbf{Mallet:}

\textbf{DBpedia Spotlight:}

\textbf{CRF implementations:}

\textbf{General article for text engineering:}

Stanford CRF-based named entity recognition system.

Apache OpenNLP.

\section{Miscellanea}

\textbf{On silver bullets:} A large number of different ideas and
algorithms have been used to do information extraction.  Are there
silver bullet ideas that can be used to solve the information
extraction problem at one go?  It's certainly true that there are some
\emph{very good} ideas that let us go far with surprisingly little
effort.  However, human language has evolved over many thousands of
years, and has a large amount of inherent complexity, a complexity
which is in part reflected by the many years we spend learning to
understand speech, and by the amusing ``obvious'' (actually, not at
all) mistakes that children often make.  It may be that any system for
extracting information thus has to reflect this inherent complexity.
We should aim for big wins, but expecting a silver bullet may be
expecting too much.

\textbf{What's the connection to entity-relationship diagrams, if
  anything?}

\end{document}